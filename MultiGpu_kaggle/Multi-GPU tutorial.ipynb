{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is an example of using multi-gpu of tensor flow.\n",
    "I used cifar-10 model and if you want to increase the performance, you have to  adjust the image size, use inception model or vgg model, or you will get better performance if you use image argumentation. <br></br>\n",
    "\n",
    "if you want to see original code [here][1]\n",
    "[1]:  https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "breed = set(df['breed'])\n",
    "n_class = len(breed)\n",
    "class_to_num = dict(zip(breed, range(n_class)))\n",
    "num_to_class = dict(zip(range(n_class), breed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "X = np.zeros((n, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, n_class), dtype=np.uint8)\n",
    "for i in tqdm(range(n)):\n",
    "    X[i] = cv2.resize(cv2.imread('train/%s.jpg' % df['id'][i]), (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    y[i][class_to_num[df['breed'][i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "NUMBER_TEST = len(df_test)\n",
    "X_test = np.zeros((NUMBER_TEST, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(NUMBER_TEST)):\n",
    "    X_test[i] = imresize(imread('../input/test/%s.jpg' % df_test['id'][i]), (IMAGE_SIZE, IMAGE_SIZE)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image argumentation\n",
    "I changed the range of pixels only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X, np.float32) / 255.\n",
    "X_test = np.array(X_test,np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar-10 model build\n",
    "you use tf.get_variable() in order to share variables across multiple GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float32\n",
    "    var = _variable_on_cpu(name,shape,\n",
    "                           tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME', name='pool1')\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                      name='norm1')\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                           strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        reshape = tf.reshape(pool2, [-1, 8*8*64])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [192, 120],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [120],\n",
    "                              tf.constant_initializer(0.0))\n",
    "        logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-gpu training\n",
    "* set parameter : if you use multi-gpu, change NUMBER_GPU value. <br></br>\n",
    "\n",
    "This is example, so I only trained 50 step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_GPU = 1\n",
    "NUMBER_TRAIN = int(np.shape(X)[0])\n",
    "MAX_STEP = 50\n",
    "BATCH_SIZE = 32\n",
    "TOWER_NAME = 'tower'\n",
    "IMAGE_SIZE = 32\n",
    "CHANNEL = 3\n",
    "NUMBER_CLASS = 120\n",
    "NUM_EPOCHS_PER_DECAY = 30.0\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.16\n",
    "INITIAL_LEARNING_RATE = 0.01\n",
    "MOVING_AVERAGE_DECAY = 0.9\n",
    "RMSPROP_DECAY =0.9\n",
    "RMSPROP_MOMENTUM = 0.9\n",
    "RMSPROP_EPSILON = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add the losses assigned to each gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tower_loss(scope, image, label):\n",
    "\n",
    "    logits = inference(image)\n",
    "    labels = tf.cast(label, tf.int64)\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    _ = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    losses = tf.get_collection('losses', scope)\n",
    "\n",
    "    total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the mean value of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "            # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image, train_label = X, y\n",
    "\n",
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0.1), trainable=False)\n",
    "    num_batches_per_epoch = (NUMBER_TRAIN / BATCH_SIZE)\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "    \n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(lr, RMSPROP_DECAY,\n",
    "                                          momentum = RMSPROP_MOMENTUM,\n",
    "                                          epsilon = RMSPROP_EPSILON)\n",
    "    \n",
    "    randidx = np.random.randint(NUMBER_TRAIN, size= BATCH_SIZE)\n",
    "    batch_xs = train_image[randidx, :]\n",
    "    batch_ys = train_label[randidx, :]\n",
    "    \n",
    "    batch_xs = tf.convert_to_tensor(batch_xs)\n",
    "    batch_ys = tf.convert_to_tensor(batch_ys)\n",
    "    \n",
    "    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue([batch_xs, batch_ys], capacity=2 * NUMBER_GPU)\n",
    "    \n",
    "    tower_grads = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        for i in range(NUMBER_GPU):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('%s_%d' % (TOWER_NAME, i)) as scope:\n",
    "                    image_batch, label_batch = batch_queue.dequeue()\n",
    "                    loss = tower_loss(scope, image_batch,label_batch)\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                    grads = optimizer.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "    grads = average_gradients(tower_grads)   \n",
    "    \n",
    "    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "                      allow_soft_placement=True,\n",
    "                      log_device_placement=False))\n",
    "    \n",
    "    sess.run(init)\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    \n",
    "    print('Learning start.')\n",
    "    for step in range(MAX_STEP):\n",
    "        loss_time = time.time()\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        duration = time.time() - loss_time\n",
    "        \n",
    "        if step % 10 == 0 :\n",
    "            num_examples_per_step = BATCH_SIZE * NUMBER_GPU\n",
    "            examples_per_sec = num_examples_per_step / duration\n",
    "            sec_per_batch = duration / NUMBER_GPU\n",
    "            \n",
    "            format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ''sec/batch)')\n",
    "            print(format_str % (datetime.now(), step , loss_value, examples_per_sec, sec_per_batch))\n",
    "    print('Learning finish.')  \n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    logits = inference(X_test)\n",
    "    softmax_logits = tf.nn.softmax(logits)\n",
    "    predict = sess.run(softmax_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for b in breed:\n",
    "    df_test[b] = predict[:,class_to_num[b]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('predict.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
